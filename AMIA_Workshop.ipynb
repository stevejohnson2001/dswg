{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDvLFJcIxaah"
   },
   "source": [
    "# 1.0 Setup\n",
    "\n",
    "Ensure that your Jupyter environment is setup correctly and import all of the data science libraries that we will need.  If some modules are missing, we will attempt to install the library but it is usually a better practice to install it in your environment directly.\n",
    "\n",
    "Also, if the data is missing, we will attempt to download it (from github) and put it in the \"/data\" subdirectory of your current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZMOIsxe_NguU"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mplot\n",
    "%matplotlib inline\n",
    "import IPython\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.debugger import set_trace\n",
    "from distutils.version import StrictVersion\n",
    "print(\"numpy version:  %s\" % np.__version__)\n",
    "print(\"pandas version:  %s\" % pd.__version__)\n",
    "print(\"matplotlib version:  %s\" % mplot.__version__)\n",
    "print(\"IPython version:  %s\" % IPython.__version__)\n",
    "print(\"seaborn version:  %s\" % sns.__version__)\n",
    "\n",
    "if StrictVersion(np.__version__) >= StrictVersion('1.13.0') and \\\n",
    "   StrictVersion(pd.__version__) >= StrictVersion('0.20.0') and \\\n",
    "   StrictVersion(mplot.__version__) >= StrictVersion('2.0.0') and \\\n",
    "   StrictVersion(IPython.__version__) >= StrictVersion('5.5.0') and \\\n",
    "   StrictVersion(sns.__version__) >= StrictVersion('0.7.0'):\n",
    "    print('\\nCongratulations, your environment is setup correctly!')\n",
    "else:\n",
    "    print('\\nEnvironment is NOT setup correctly!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AVqO021euWsf"
   },
   "outputs": [],
   "source": [
    "# Try to install the Excel reader library\n",
    "\n",
    "try:\n",
    "    import xlrd\n",
    "    print('The Excel library is installed.')\n",
    "except ImportError:\n",
    "    print('Installing the Excel library')\n",
    "    !pip install xlrd\n",
    "    import xlrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jWSv1aAZNgua"
   },
   "source": [
    "### 1.1 Check data directory\n",
    "\n",
    "See if the data exists.  If not, try to download it from github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EqZAggVouWsk"
   },
   "outputs": [],
   "source": [
    "# Find the data directory and download data if it is missing\n",
    "\n",
    "import os, shutil\n",
    "cwd = os.getcwd()\n",
    "datadir = cwd + '/data'\n",
    "\n",
    "print('Data directory is: {}'.format(datadir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7k3ztEe_Nlvs"
   },
   "outputs": [],
   "source": [
    "# See if the data exists.  If not, try to download it from github.\n",
    "if not os.path.exists(datadir+'/patients.csv'):\n",
    "    print(\"Data directory doesn't exist!\")\n",
    "    # Checkout the data from Github\n",
    "    !git clone https://github.com/stevejohnson2001/dswg\n",
    "    shutil.move('dswg/data','.')  # Move the checked-out files into the /data directory\n",
    "    shutil.rmtree('dswg')  # Remove the version control (git) information\n",
    "print('Data directory contains:\\n',os.listdir(datadir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZA3W-hhxkpV"
   },
   "source": [
    "# 2.0 Read in the data\n",
    "\n",
    "Now that we have all the libraries installed and the data is available, lets try to read it into Pandas DataFrames.  \n",
    "\n",
    "The first thing we will do is define a Data Dictionary (dd) that describes our expectations for the data.  It includes data types for the columns as well is information about whether the column is required or optional.  The data is read into a dictionary of Dataframes (data) and also assigned to  variables (patients, encounters, etc) for convenience.\n",
    "\n",
    "We will convert dates and other fields to the proper format when later when we do data preparation.\n",
    "\n",
    "The dataset is synthetic data generated by the Synthea project (https://github.com/synthetichealth/synthea).  Synthea creates realistic (but not real) EHR-like data that we can use for demonstrating the techniques of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "U3ymTsiKuWsn"
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "dd = {}\n",
    "\n",
    "dd['patients'] = {'pat_id':     {'type': np.str, 'required':True},  \n",
    "                  'birth_date': {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                  'death_date': {'type': np.datetime64, 'format': '%Y-%m-%d' }, \n",
    "                  'ssn':        {'type': np.str},\n",
    "                  'drivers':    {'type': np.str},\n",
    "                  'passport':   {'type': np.str},\n",
    "                  'prefix':     {'type': np.str},\n",
    "                  'first':      {'type': np.str, 'required':True},\n",
    "                  'last':       {'type': np.str, 'required':True},\n",
    "                  'suffix':     {'type': np.str},\n",
    "                  'maiden':     {'type': np.str},\n",
    "                  'marital':    {'type': np.str},\n",
    "                  'race':       {'type': np.str},\n",
    "                  'ethnicity':  {'type': np.str},\n",
    "                  'gender':     {'type': np.str, 'required':True},\n",
    "                  'birthplace': {'type': np.str},\n",
    "                  'address':    {'type': np.str, 'required':True},\n",
    "                  'prior_opioid_abuse_diag': {'type': np.int}\n",
    "                  }\n",
    "dd['encounters'] = {'enc_id':                 {'type': np.str, 'required':True}, \n",
    "                    'enc_date':               {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                    'enc_pat_id':             {'type': np.str, 'required':True},\n",
    "                    'enc_code':               {'type': np.str, 'required':True},\n",
    "                    'enc_description':        {'type': np.str, 'required':True},\n",
    "                    'enc_reason_code':        {'type': np.str},\n",
    "                    'enc_reason_description': {'type': np.str}\n",
    "                   }\n",
    "dd['observations'] = {'obs_date':        {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                      'obs_pat_id':      {'type': np.str, 'required':True},\n",
    "                      'obs_enc_id':      {'type': np.str, 'required':True},\n",
    "                      'obs_code':        {'type': np.str, 'required':True},\n",
    "                      'obs_description': {'type': np.str, 'required':True},\n",
    "                      'obs_value':       {'type': np.str},\n",
    "                      'obs_units':       {'type': np.str}\n",
    "                     }\n",
    "dd['medications'] = {'med_start_date':         {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                     'med_stop_date':          {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':False},\n",
    "                     'med_pat_id':             {'type': np.str, 'required':True},\n",
    "                     'med_enc_id':             {'type': np.str, 'required':True},\n",
    "                     'med_code':               {'type': np.str, 'required':True},\n",
    "                     'med_description':        {'type': np.str, 'required':True},\n",
    "                     'med_reason_code':        {'type': np.str},\n",
    "                     'med_reason_description': {'type': np.str},\n",
    "                     'med_days_supply':        {'type': np.int}\n",
    "                     }\n",
    "dd['conditions'] =  {'cond_start_date':         {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                     'cond_stop_date':          {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':False},\n",
    "                     'cond_pat_id':             {'type': np.str, 'required':True},\n",
    "                     'cond_enc_id':             {'type': np.str, 'required':True},\n",
    "                     'cond_code':               {'type': np.str, 'required':True},\n",
    "                     'cond_description':        {'type': np.str, 'required':True}\n",
    "                     }\n",
    "\n",
    "\n",
    "# Display the data dictionary\n",
    "# Use HTML to make it look a little nicer\n",
    "\n",
    "for name, tbl_dd in dd.items():\n",
    "    display(HTML('<h2>{}</h2>'.format(name)))\n",
    "    display(pd.DataFrame(tbl_dd).fillna('').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "n3-p3h8lP7jR"
   },
   "outputs": [],
   "source": [
    "# Loop through each of the file defintions in our data dictionary\n",
    "data = {}\n",
    "for f in dd:\n",
    "    m = dd[f]\n",
    "    col_names = list(m.keys())\n",
    "    data[f] = pd.read_csv(datadir + '/{}.csv'.format(f), dtype=str, index_col=False, header=0, \\\n",
    "                          names=col_names, keep_default_na=False)\n",
    "\n",
    "# Assign data to local variables for convenience\n",
    "patients = data['patients']\n",
    "encounters = data['encounters']\n",
    "observations = data['observations']\n",
    "medications = data['medications']\n",
    "conditions = data['conditions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oqtssqpRNgun"
   },
   "source": [
    "### 2.1 Load the list of opioid medications\n",
    "\n",
    "It will be important to know which of the medications that are prescribed are considered opioids.  The UMLS VSAC maintains value set lists of which medications are considered opioids.  You can download the current list by going to https://vsac.nlm.nih.gov/ and searching for opioid value sets.  We have downloaded the list called \"All prescribable opioids used for pain control including Inactive Medications\" (oid 1.3.6.1.4.1.6997.4.1.2.234.999.3.2) into the data directory.  We will read the Excel file and pull out the codes from the second sheet in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6ygbd4wpNguo"
   },
   "outputs": [],
   "source": [
    "# Get Opioid code list from VSAC\n",
    "# oid 1.3.6.1.4.1.6997.4.1.2.234.999.3.2\n",
    "xl = pd.ExcelFile(datadir + '/AllPrescribableOpioidsUsedForPainControlIncludingInactiveMedications.xlsx')\n",
    "df = xl.parse(\"Code List\", skiprows=12)\n",
    "display(df.head(10))\n",
    "opioids_rxnorm = list(df['Code'].astype(np.str))  # Make sure the codes are treated as strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwLnaiLiLjba"
   },
   "source": [
    "# 3.0 Exploratory Data Analysis - Part 1\n",
    "\n",
    "Start by looking at the data to see what types of values are in each variables, the relationships and get a feel for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49JYp6fJNgur"
   },
   "source": [
    "### 3.1 Start by displaying the data as DataFrames\n",
    "\n",
    "Displaying the first few rows of the data is a good way to look for obvious issues before working with the data in more detail.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LvwqJrjoNgur"
   },
   "outputs": [],
   "source": [
    "# Loop through each of the file defintions in our data dictionary\n",
    "for f in dd:\n",
    "    m = dd[f]\n",
    "    display(HTML('<h3>{}, {} records</h3>'.format(f,len(data[f]))))\n",
    "    display(data[f].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also display the last few rows using the .tail() function.\n",
    "patients.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Oxb4DG_nMDwv"
   },
   "outputs": [],
   "source": [
    "# Look at the categorical variables\n",
    "\n",
    "sns.countplot(x='race', data=patients)\n",
    "plt.title('Race')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='obs_code',data=observations)\n",
    "plt.title('Observation Codes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observations are hard to read, lets try a bar plot\n",
    "c = observations['obs_description'].value_counts(ascending=True)\n",
    "fig = plt.figure(figsize=(8,24))\n",
    "c.plot(kind=\"barh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Continuous Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XOOgSrclNguy"
   },
   "outputs": [],
   "source": [
    "# For continuous variables, we can graph the distribution\n",
    "\n",
    "w = observations[observations['obs_code']=='29463-7']   # Find all of the \"weight\" observations\n",
    "weights = w['obs_value'].astype(np.float)\n",
    "mean = np.mean(weights)\n",
    "print('Average weight: ',mean)\n",
    "\n",
    "# Plot the distribution\n",
    "sns.distplot(weights)\n",
    "plt.xlabel(\"WEIGHT (kg)\")\n",
    "plt.show()   # If you don't explicitly \"show\" the plot, Jupyter will automatically show the last plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7lz_KGneNgu0"
   },
   "outputs": [],
   "source": [
    "# As an Exercise, have participants graph the Height of patients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-xiAaMhNgu2"
   },
   "source": [
    "# 4.0 Data Preparation\n",
    "\n",
    "Now that we know a little about our data, we can begin to prepare it for analysis.  We need to:\n",
    "1. Find data that is not formatted correctly\n",
    "2. Deal with missing data\n",
    "\n",
    "In all of these cases, we will have to decide what to do with the bad data.  We can:\n",
    "1. Delete the data\n",
    "2. Impute a reasonable value for the missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xBTLtHu4Ngu3"
   },
   "source": [
    "### 4.1 Data Quality Checks\n",
    "\n",
    "Start by running a few data quality checks against all of the data.  For this workshop, we will only look at missing data or incorrectly formatted data but with a real data science project you would also want to check that the relationships between all of the data elements makes sense.  For example, you'd ensure that the `birth_date` is less than today.\n",
    "\n",
    "The functions below attempt to convert the data (which we read in as simple strings) to the data format that we defined for each variable.  That's how we will tell if it is in the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Dn9X4BCjNgu8"
   },
   "outputs": [],
   "source": [
    "# Find data that is not formatted correctly\n",
    "\n",
    "def parse_date(dt,fmt):\n",
    "    if type(dt) == str and (dt == ''):\n",
    "        return dt\n",
    "    try:\n",
    "        return pd.to_datetime(dt,format=fmt)\n",
    "    except:\n",
    "        return np.datetime64('NaT')\n",
    "\n",
    "def parse_int(num):\n",
    "    if type(num) == str and (num == ''):\n",
    "        return num\n",
    "    try:\n",
    "        return int(num)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Loop through our Data Dictionary\n",
    "for name, tbl_dd in dd.items():\n",
    "    display(HTML('<h2>{}</h2>'.format(name)))\n",
    "    d = data[name]\n",
    "    for field_name, field in tbl_dd.items():\n",
    "        col = d[field_name]\n",
    "        field['DQ'] = {}\n",
    "        field['DQ']['missing'] = len(np.where(col == '')[0])\n",
    "\n",
    "        if field['type'] == np.datetime64:\n",
    "            if 'format' in field:\n",
    "                fmt = field['format']\n",
    "            else:\n",
    "                fmt = '%Y-%m-%d'   # Default date format if not specified\n",
    "                \n",
    "            d[field_name] = col.apply(lambda x: parse_date(x,fmt))\n",
    "            field['DQ']['format_errors'] = col.isnull().sum()\n",
    "        elif field['type'] == np.int:\n",
    "            d[field_name] = col.apply(lambda x: parse_int(x))\n",
    "            field['DQ']['format_errors'] = col.isnull().sum()\n",
    "        elif field['type'] == np.str:\n",
    "            pass # Everything is valid syntax\n",
    "            field['DQ']['format_errors'] = 0\n",
    "            \n",
    "    # Show the Data Quality information\n",
    "    display(pd.DataFrame(dd[name]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Drop unwanted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove some of the data that we won't need to use for the workshop\n",
    "# It will make some of the screens easier to read\n",
    "# Put it in a try block in case we've already dropped the columns\n",
    "try:\n",
    "    patients.drop(['maiden','passport','drivers','prefix','suffix','ssn','first','last'],axis=1,inplace=True)\n",
    "except:\n",
    "    pass\n",
    "display(patients.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Print the Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XY46r3jENgvB"
   },
   "outputs": [],
   "source": [
    "# Print Data Dictionary\n",
    "# TODO: Move this\n",
    "for name, tbl_dd in dd.items():\n",
    "    display(HTML('<h2>{}</h2>'.format(name)))\n",
    "    list(dd[name].keys())\n",
    "    d = pd.DataFrame(dd[name]).fillna('')\n",
    "    d = d[list(dd[name].keys())]\n",
    "    d = d.replace(np.str,'str')\n",
    "    d = d.replace(np.datetime64,'datetime')\n",
    "    d = d.replace(np.int,'int')\n",
    "    d = d.loc[['type','format','required'],:]\n",
    "    if name == 'patients':\n",
    "        d = d.drop(['maiden','passport','drivers','prefix','suffix','ssn','first','last'],axis=1)\n",
    "    for fname, field in d.items():\n",
    "        if fname.endswith('date'):\n",
    "            field['type'] = 'datetime'\n",
    "    display(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Remove rows with missing data\n",
    "\n",
    "For this workshop, we will only deal with missing data by dropping it.  If a row has missing or bad formatted data and the field is required, we will drop the entire row.\n",
    "\n",
    "An alternative is to try to fix the data by imputing a reasonable value for it, such as the column .mean()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IWwIx3NYNgvF"
   },
   "outputs": [],
   "source": [
    "# The .isnull() functions are used to find bad data\n",
    "# The .any() function returns the columns that contain any True values\n",
    "\n",
    "display(patients.isnull().head(5))\n",
    "display(patients.isnull().any())\n",
    "\n",
    "# Let's get a list of all of the columns with some missing data \n",
    "missing_cols=patients.columns[patients.isnull().any()]\n",
    "print(missing_cols)\n",
    "\n",
    "# We can see how many cells have missing data for each column\n",
    "patients[missing_cols].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mo0LqZOeNgvO"
   },
   "outputs": [],
   "source": [
    "# Drop missing or incorrectly formated data\n",
    "\n",
    "# TODO: Do for all Data\n",
    "\n",
    "# Get the row numbers (index) of each of the rows with missing data\n",
    "missing = patients[patients.isnull().any(axis=1)].index\n",
    "print(\"Missing = \",missing)\n",
    "print('Before patients shape = ',patients.shape)\n",
    "patients.drop(missing,inplace=True)\n",
    "\n",
    "# Make sure the rows are gone\n",
    "missing = patients[patients.isnull().any(axis=1)].index\n",
    "print(\"Missing = \",missing)\n",
    "print('After patients shape = ',patients.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVVDeJqpMSK6"
   },
   "source": [
    "## 4.1 Transform the Data\n",
    "\n",
    "Use the power of Pandas Dataframes to transform the data.  Add new columns as calculations from existing columns, join the data together and get it into the format you need for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Mu-qPMysNgvP"
   },
   "outputs": [],
   "source": [
    "# Create the working dataframe\n",
    "# TODO cleanup\n",
    "\n",
    "df = patients\n",
    "df['age'] = round((pd.Timestamp.today() - pd.to_datetime(patients['birth_date'])).dt.days/365)\n",
    "df['adult'] = np.where(df['age'] >= 18, 1, 0)\n",
    "\n",
    "# Determine which patients have ever overdosed\n",
    "# Use a set to eliminate duplicates\n",
    "patients_that_overdosed = set(encounters[encounters['enc_reason_code']=='55680006']['enc_pat_id'])  # Overdose\n",
    "df['overdose'] = np.where(df['pat_id'].isin(patients_that_overdosed), 1, 0)\n",
    "\n",
    "# Determine which patients have died from an overdose\n",
    "# Uses binary indexing\n",
    "patients_overdose_deaths = set(observations[(observations['obs_code'] == '69453-9') &   # Death\n",
    "                                (observations['obs_value'].str.contains('overdose'))]['obs_pat_id'])\n",
    "\n",
    "# Determine which patients were ever prescribed opioids\n",
    "patients_prescribed_opioids = set(medications[medications['med_code'].isin(opioids_rxnorm)]['med_pat_id'])  # Opioids\n",
    "df['prescribed_opioids'] = np.where(df['pat_id'].isin(patients_prescribed_opioids), 1, 0)\n",
    "\n",
    "print('Num patients prescribed opioids = {}, Num overdoses = {}, Num overdose deaths = {}'\n",
    "         .format(len(patients_prescribed_opioids),len(patients_that_overdosed),len(patients_overdose_deaths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXdxuyM5MeIG"
   },
   "source": [
    "### 4.2.1 Compute the days_supply variable\n",
    "\n",
    "We want to compute how many days supply of a medication a patient was prescribed at discharge.  The approach we will use is that for each encounter, we will find all of the medications associated with the encounter.  We will look for medicates that are opioids and find the largest days supply for that encounter and store the result in the 'opioid_discharge_days_supply' column.\n",
    "\n",
    "This is most easily accomlished using a function that defines the logic and the \".apply\" DataFrame function the will iterate over each row in a DataFrame, call the function and store the result back in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HF7nIZwfNgvS"
   },
   "outputs": [],
   "source": [
    "# Define the function that will perform to logic of compute the discharge opioid days supply\n",
    "from random import random\n",
    "# TODO Fix in CreateData\n",
    "def get_days_supply(pat_id):\n",
    "    enc_meds = medications[medications['med_pat_id'] == pat_id]\n",
    "    enc_opioid_meds = enc_meds[enc_meds['med_code'].isin(opioids_rxnorm)]\n",
    "    max = 0\n",
    "    if len(enc_opioid_meds) > 0:\n",
    "        try:\n",
    "            max = int(enc_opioid_meds['med_days_supply'].max())\n",
    "        except ValueError:\n",
    "            max = 0\n",
    "    return int(max)\n",
    "\n",
    "# Apply the function to each row (Note: this can take a little while to finish)\n",
    "df['opioid_discharge_days_supply'] = df.apply(lambda x: get_days_supply(x['pat_id']), axis=1)\n",
    "\n",
    "# Display the first 5 entries that have a non-zero days supply, just to check our logic\n",
    "df[df['opioid_discharge_days_supply'] > 0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2An5P7D0NgvU"
   },
   "source": [
    "# 5.0 Explore the Data - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygRWbg6_QHur"
   },
   "source": [
    "### 5.1 Outcome variable\n",
    "\n",
    "Now that we have created some new variables, lets take a look at how the outcome variable is associated with our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8w6XYA9eNgva"
   },
   "outputs": [],
   "source": [
    "# See who overdosed from prescribed opioids by computing the intersection\n",
    "\n",
    "overlap = patients_that_overdosed.intersection(patients_prescribed_opioids)\n",
    "\n",
    "print('Num that overdose = {}, Num that were prescribed opioids = {}, overlap = {}'.format(\\\n",
    "    len(patients_that_overdosed),len(patients_prescribed_opioids),len(overlap)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwbdILHlNgvb"
   },
   "source": [
    "How many patients overdosed?\n",
    "\n",
    "Since we store 'overdose' as a 0 or 1, we can just use the mean function to compute what percent of the population overdosed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "D6W9vKMeNgvb"
   },
   "outputs": [],
   "source": [
    "ov = patients[patients['pat_id'].isin(patients_prescribed_opioids)]['overdose']\n",
    "display(ov.value_counts())\n",
    "print('Percent that overdosed: {0:.2f}%'.format(ov.mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IZWAMdv1Ngvd"
   },
   "outputs": [],
   "source": [
    "patients.groupby('overdose').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "M2AvZvFJNgvh"
   },
   "outputs": [],
   "source": [
    "# What was the mean number of days_supply for patients that overdosed and were prescribed opioids?\n",
    "\n",
    "ct = pd.crosstab(df['prescribed_opioids'],df['opioid_discharge_days_supply'])\n",
    "ct.iloc[1][1:].plot()\n",
    "#sns.factorplot(data=ct[0], x='')\n",
    "df[['overdose','opioid_discharge_days_supply']].groupby('overdose').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQTjt_xaNgvj"
   },
   "source": [
    "### 5.3 Graph the patient variables against the outcome\n",
    "\n",
    "Let's see if gender, race and age are associated with the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "v6y5oqBmNgvk"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(patients['gender'],patients['overdose']).plot(kind='bar')\n",
    "plt.title('Overdose by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Overdose?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c3Cw7F0mNgvm"
   },
   "outputs": [],
   "source": [
    "# Number of Male vs Female overdoses\n",
    "display(pd.crosstab(patients['gender'],patients['overdose'],margins=True))\n",
    "display(pd.crosstab(patients['race'],patients['overdose'],margins=True))\n",
    "display(pd.crosstab(patients['race'],patients['overdose'],normalize='index'))\n",
    "pd.crosstab(patients['ethnicity'],patients['overdose'],normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6Ii3ZGpENgvo"
   },
   "outputs": [],
   "source": [
    "pd.crosstab(patients['race'],patients['overdose']).plot(kind='bar')\n",
    "plt.title('Overdose by Race')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Overdose?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R__EWwiLNgvp"
   },
   "source": [
    "We have a pretty uniform distribution of ages in the patient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "itxkiNCNNgvp"
   },
   "outputs": [],
   "source": [
    "patients['age'].hist()\n",
    "plt.title('Histogram of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c4IEbm9UNgvr"
   },
   "outputs": [],
   "source": [
    "# Graph age_at_visit with probability of overdose\n",
    "# TODO: FIX\n",
    "\n",
    "ct = pd.crosstab(df['age'],df['overdose'],normalize='index')[1]\n",
    "\n",
    "#ct.hist()\n",
    "ct.plot()\n",
    "plt.title('Histogram of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Probability of Overdose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcaI-FaONgvs"
   },
   "source": [
    "### 5.4 Grouping by a variable\n",
    "\n",
    "We often want to group related rows together and then count the number rows of each type or find the mean of a variable for each row type.\n",
    "\n",
    "For example, lets count how many encounters each patient has over the timeframe of the data.  We will use the `groupby` function to group on a set of variables.  The operation returns a `groupby` object which doesn't actually group the data but instead acts like a set of instructions telling the DataFrame how to group itself.  We need to apply another function, such as size(), mean() or sum(), to the groups to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EAW0QqbkLrJh"
   },
   "outputs": [],
   "source": [
    "# How many encounters does each patient have?\n",
    "\n",
    "encs = encounters.groupby(['enc_pat_id']).size()\n",
    "\n",
    "#display(encs[0:5])\n",
    "#display(type(encs))\n",
    "\n",
    "# We can store that information directly into the patients DataFrame since `encs` is indexed by the pat_id\n",
    "patients = patients.set_index('pat_id')\n",
    "patients['num_encounters'] = encs\n",
    "patients = patients.reset_index()\n",
    "\n",
    "#display(patients.head(5))\n",
    "\n",
    "sns.lmplot(data=patients,x='age',y='num_encounters',hue='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1X5aAmRPNgvu"
   },
   "source": [
    "## For those that overdose, what is the days_supply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_g69wExxNgvu"
   },
   "outputs": [],
   "source": [
    "display(df[df['overdose']==1].mean())\n",
    "df[df['prescribed_opioids']==1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P-iNFJ0dNgvx"
   },
   "source": [
    "### What are the primary reasons for visit for those that overdosed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5KOtf0EZNgvx"
   },
   "outputs": [],
   "source": [
    "encounters[encounters['enc_pat_id'].isin(patients_that_overdosed)]['enc_reason_description'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ItOHd2-uNgv0"
   },
   "source": [
    "## For patients that overdosed, what was the most frequent condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7uDGsGGoNgv1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conditions[conditions['cond_pat_id'].isin(patients_that_overdosed)]['cond_description'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1l2tXR5hNgv3"
   },
   "source": [
    "### Get an idea of how a patient progresses through their healthcare\n",
    "\n",
    "When exploring the data, it helps to visualize what is happening across time.  You can create small functions within the Jupyter notebook and reuse them further down in the notebook.  In this case, we are looping through the encounter data for a patient and print all of the medications and labs (observations) that are associated with the patient.  The function 'display_trajectory' is passed the id for a patient and then prints the information.  We can use this later to further examine data or debug things we don't understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kNlJlgaCY-O2"
   },
   "outputs": [],
   "source": [
    "pt_id = '3eaed230-1c60-4221-a96c-f6af5d871072'\n",
    "#pt = patients.query('pat_id == @pt_id')\n",
    "#pt = patients[patients.pat_id == pt_id].iloc[0]\n",
    "#pt = patients.loc[pt_id]\n",
    "\n",
    "def display_trajectory(df,pt_id):\n",
    "    pt = patients[patients['pat_id']==pt_id]\n",
    "\n",
    "    display(pt)\n",
    "    encs = encounters[encounters.enc_pat_id == pt_id]\n",
    "    #print(encs.shape)\n",
    "    for i, e in encs.iterrows():\n",
    "        #dt = df[df['pat_id']==e['enc_pat_id']].iloc[0]\n",
    "        print('  {:%Y-%m-%d}: {} ({}) ({})'.format(e['enc_date'], e['enc_description'], \\\n",
    "                         e['enc_code'], e['enc_reason_description']))\n",
    "        meds = medications[medications['med_enc_id'] == e['enc_id']]\n",
    "        for j, m in meds.iterrows():\n",
    "            print('     MED: {:%Y-%m-%d}: {} ({}) days_supply={}'.format(m['med_start_date'],  \\\n",
    "                                            m['med_description'], m['med_code'], m['med_days_supply']))\n",
    "        labs = observations[observations['obs_enc_id'] == e['enc_id']]\n",
    "        for k, l in labs.iterrows():\n",
    "            print('     LAB: {:%Y-%m-%d %H:%M}: {} ({}) {} {}'.format(l['obs_date'], l['obs_description'], l['obs_code'], l['obs_value'], l['obs_units']))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qGRUik5KNgv5"
   },
   "outputs": [],
   "source": [
    "display_trajectory(df,patients.iloc[0].pat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dIhLzt0XNgv7"
   },
   "outputs": [],
   "source": [
    "# Who were the patients that overdosed?\n",
    "\n",
    "pt = df[df['pat_id'].isin(patients_that_overdosed)]\n",
    "\n",
    "pt.head(10)\n",
    "#display(pd.DataFrame([pt['first'],pt['last'],pt['marital'],pt['race'],pt['gender'],pt['ethnicity']]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gF98rMhDNgv9"
   },
   "outputs": [],
   "source": [
    "# Display the trajectory of one of those patients\n",
    "pt_id = list(patients_that_overdosed)[1]\n",
    "display_trajectory(df,pt_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V59KYvYnNgwD"
   },
   "source": [
    "# 6.0 Modeling\n",
    "\n",
    "We will be using the `scikit-learn` package, which is an open-source library that provides a robust set of machine learning algorithms for Python. It is built upon the core Python scientific stack and has a simple, consistent interface.\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-ME24ePzpzIM/UQLWTwurfXI/AAAAAAAAANw/W3EETIroA80/s1600/drop_shadows_background.png\" width=\"800px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OfMC90NXNgwD"
   },
   "outputs": [],
   "source": [
    "# Load the modules\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import random as rnd\n",
    "from random import random, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C1ourAEvNgwE"
   },
   "source": [
    "Most of the models in scikit-learn require the categorical variables be turned into numeric variables.  There are two approaches to this:\n",
    "1. One Hot Encoding - each item in the categorical variable is turned into its own variable represetnting the presence or abscence of that item.  For example, 'Gender' would turn into 2 variables:  'Gender_M' and 'Gender_F'\n",
    "2. Label Encoding - assign an integer to each item.  For the 'Gender' variable, 0 might mean Male and 1 might mean Female.\n",
    "\n",
    "We will use the `LabelEncoder` transformation to change categorical variables into integers.  As a convenience, we can also keep the original (human readable) variable in the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "32uGgjFhNgwF"
   },
   "outputs": [],
   "source": [
    "# Let's use the following variables as our initial set of predictors\n",
    "cat_cols = ['gender', 'marital', 'race', 'ethnicity']\n",
    "cat_cols_encoded = [c + '_encoded' for c in cat_cols]\n",
    "numeric_cols = ['prior_opioid_abuse_diag', 'age', 'opioid_discharge_days_supply']\n",
    "pred_cols = numeric_cols + cat_cols_encoded\n",
    "target_col = 'overdose'\n",
    "all_cols = cat_cols+numeric_cols+[target_col]\n",
    "\n",
    "df_opioids = df[df['prescribed_opioids'] == 1]\n",
    "print(df_opioids.shape)\n",
    "\n",
    "# Encode the categorical variables\n",
    "dfe = df_opioids[cat_cols]\n",
    "print(dfe.shape)\n",
    "# Replace missing data with an 'Unknown' category so the missing data will also be encoded\n",
    "dfe = dfe.replace(np.NaN,'Unknown')\n",
    "\n",
    "# Encode the categorical variables\n",
    "encoded = dfe.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "\n",
    "# Append the non-categorical variables and the encoded variables into a single Dataframe\n",
    "# Name the new variables as <name>_encoded\n",
    "dfe = pd.concat([df_opioids[all_cols], encoded.add_suffix('_encoded')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jqhgzv5eNgwG"
   },
   "outputs": [],
   "source": [
    "display(df_opioids.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "O1u-uqyKNgwH"
   },
   "outputs": [],
   "source": [
    "# df_opioids.loc[:,'prior_opioid_abuse_diag'] = df_opioids['overdose'].apply(lambda x: x if random() > .90 else 1)\n",
    "# df_opioids.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AfY5EBR2NgwJ"
   },
   "outputs": [],
   "source": [
    "# Lets try to build a model using this set of variables\n",
    "pred_cols = ['age', 'opioid_discharge_days_supply', \n",
    "             'prior_opioid_abuse_diag', \\\n",
    "             'gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded']\n",
    "#pred_cols = ['prior_abuse_diag', 'adult', 'age_at_visit', 'opioid_discharge_days_supply', \\\n",
    "#             'gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded']\n",
    "\n",
    "LR_pred_cols = pred_cols\n",
    "X = dfe[pred_cols].as_matrix()\n",
    "y = dfe['overdose'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nNFfashzNgwL"
   },
   "source": [
    "To remind us of the definitions of recall (sensitivity) and precision (positive predictive value), here is a graphic from Wikipedia:\n",
    "\n",
    "![Confusion Matrix](Sensitivity-Wikipedia.png)\n",
    "\n",
    "To quote from Scikit Learn:\n",
    "\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "Accuracy is (tp + tn) / N.\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n",
    "\n",
    "The support is the number of occurrences of each class in y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PtyA-VfANgwM"
   },
   "outputs": [],
   "source": [
    "# Let's try a simple logistic regression model to see how predictive our data is\n",
    "\n",
    "# https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "# \n",
    "\n",
    "LR = LogisticRegression()\n",
    "result = LR.fit(X,y)\n",
    "\n",
    "expected = y\n",
    "predicted = LR.predict(X)\n",
    "\n",
    "print('\\nClassification Report\\n',metrics.classification_report(expected, predicted))\n",
    "print('\\nConfusion Matrix\\n',metrics.confusion_matrix(expected, predicted))\n",
    "print('\\nAccuracy score =',metrics.accuracy_score(expected, predicted))\n",
    "print('\\nAUC score =',metrics.roc_auc_score(expected, predicted))\n",
    "print('\\nf1 score =',metrics.f1_score(expected, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8wgaaqTRNgwO"
   },
   "outputs": [],
   "source": [
    "# Lets graph an ROC curve for the model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "auc = roc_auc_score(y, LR.predict(X))\n",
    "\n",
    "probs = LR.predict_proba(X)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y, probs)\n",
    "tpr[1] = tpr[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "VGsqAL2dNgwP"
   },
   "outputs": [],
   "source": [
    "# Now lets try it with Dummy Variables (One Hot Coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTDMermuNgwR"
   },
   "source": [
    "# 7.0 Model Evaluation\n",
    "\n",
    "We built a Logistic Regression model that was moderatly predictive of an overdose.  But we trained the model and tested it with our entire data set, which isn't exactly a fair test of the model.\n",
    "\n",
    "We can split our dataset into a training dataset and a test dataset.  After we train the model using just the training dataset, we will evaluate the model using the test dataset which has data that the model has never seen before.\n",
    "\n",
    "scikit-learn has a nice function called `train_test_split` that randomly splits our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XvQxMnPCNgwU"
   },
   "outputs": [],
   "source": [
    "# Create the training and test datasets\n",
    "train, test = train_test_split(dfe, test_size=0.3, random_state=987)\n",
    "\n",
    "X_train = train[pred_cols].as_matrix()\n",
    "y_train = train['overdose'].as_matrix()\n",
    "X_test = test[pred_cols].as_matrix()\n",
    "y_test = test['overdose'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3bBHpq7sNgwV"
   },
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "result = LR.fit(X_train,y_train)\n",
    "print(LR)\n",
    "expected = y_train\n",
    "predicted = LR.predict(X_train)\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "expected = y_test\n",
    "predicted = LR.predict(X_test)\n",
    "\n",
    "print('\\nClassification Report\\n',metrics.classification_report(expected, predicted))\n",
    "print('\\nConfusion Matrix\\n',metrics.confusion_matrix(expected, predicted))\n",
    "print('\\nAccuracy score =',metrics.accuracy_score(expected, predicted))\n",
    "print('\\nAUC score =',metrics.roc_auc_score(expected, predicted))\n",
    "print('\\nf1 score =',metrics.f1_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Graph an ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rwtqadTpNgwX"
   },
   "outputs": [],
   "source": [
    "# Lets graph an ROC curve for the model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "auc = roc_auc_score(y_test, LR.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, LR.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Model (train/test split)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OpllrCeUNgwY"
   },
   "source": [
    "### 7.3 Cross Validation\n",
    "\n",
    "Splitting the data into a training dataset and a test dataset has a drawback in that it doesn't allow us to train the model on all of the data.  Another approach is to use k-fold cross validation where we randomly divide the data into k equal sized subsets and then train the model on the remaining k-1 segments of the data and test it on the data that we held out.  The process is repeated k times so that in the end all of the data is used at some point to train the model.  The k model results are averaged to produce a single estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wS0qoXm4NgwY"
   },
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=387)\n",
    "results = model_selection.cross_val_score(LR, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "print('Model score = %.4f (%.4f)' %(results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8MamwnKGNgwa"
   },
   "source": [
    "### 7.4 Testing many models\n",
    "\n",
    "The nice thing about scikit-learn is that most of the models have similar calling signatures so we can just create a list of all of the models we are interested in testing and then invoke each model in a for loop.\n",
    "\n",
    "We can train each model in turn and then use k-fold cross validate to estimate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sBFel0DZNgwb"
   },
   "outputs": [],
   "source": [
    "# Cross validation test harness \n",
    "\n",
    "# Make this into a function so we can reuse it later\n",
    "\n",
    "def eval_models(X, y):\n",
    "    seed = 543\n",
    "    # prepare models\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression(class_weight='balanced')))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    #models.append(('SVM', SVC()))\n",
    "    models.append(('RF', RandomForestClassifier(n_estimators=100, class_weight='balanced')))\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    names = []\n",
    "    #scoring = 'accuracy'\n",
    "    scoring = 'roc_auc' # others include: 'accuracy', 'f1', 'roc_auc', \n",
    "                        # or found here: http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        \n",
    "    return results, names\n",
    "\n",
    "# Execute the function\n",
    "results, names = eval_models(X_train,y_train)  #TODO:  X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zMRXlrfLNgwc"
   },
   "source": [
    "### 7.5 Graphically viewing model performance using a boxplot\n",
    "\n",
    "None of the models do very well.  LogisticRegression does the best but most of the models have quite a bit of variance in their scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gGzeEwKuNgwd"
   },
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "def plot_perf(names,results):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    plt.ylabel('roc_auc')\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()\n",
    "    \n",
    "plot_perf(names,results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6 Model Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-GzcVKf6Ngwe"
   },
   "outputs": [],
   "source": [
    "# Can we improve the model by adding some variables?\n",
    "new_cols = ['prior_opioid_abuse_diag',  'age',  \\\n",
    "             'gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded']\n",
    "print(new_cols)\n",
    "\n",
    "X = dfe[new_cols].as_matrix()\n",
    "y = dfe['overdose'].as_matrix()\n",
    "\n",
    "X_train = train[new_cols].as_matrix()\n",
    "y_train = train['overdose'].as_matrix()\n",
    "X_test = test[new_cols].as_matrix()\n",
    "y_test = test['overdose'].as_matrix()\n",
    "\n",
    "results, names = eval_models(X,y)\n",
    "\n",
    "plot_perf(names,results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7 Assess Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EAIzgduZNgwf"
   },
   "outputs": [],
   "source": [
    "# Display relative importance of each variable\n",
    "pred_cols = ['gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded', 'age', \n",
    "               'prior_opioid_abuse_diag', 'opioid_discharge_days_supply']\n",
    "model = RandomForestClassifier()\n",
    "model.fit(dfe[pred_cols], dfe['overdose'])\n",
    "\n",
    "# display the relative importance of each attribute\n",
    "display(pd.DataFrame(list(zip(pred_cols,model.feature_importances_))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8 Predict the risk for new patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Xs-2q7g3Ngwh"
   },
   "outputs": [],
   "source": [
    "# Show how to use the resulting model to predict opioid overdose\n",
    "# age, opioid_discharge_days_supply, prior_opioid_abuse_diag, gender (F), marital (M), race (white), ethnicity (english)\n",
    "new_patient = [45,10,1,0,1,4,7]\n",
    "\n",
    "pred = LR.predict(np.asmatrix(new_patient))\n",
    "if pred[0] == 0:\n",
    "    print('Patient has no overdose risk.')\n",
    "elif pred[0] == 1:\n",
    "    print('Patient has overdose risk.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "AMIA_Workshop_v14.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
