{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDvLFJcIxaah"
   },
   "source": [
    "# 1.0 Setup\n",
    "\n",
    "Ensure that your Jupyter environment is setup correctly and import all of the data science libraries that we will need.  If some modules are missing, we will attempt to install the library but it is usually a better practice to install it in your environment directly.\n",
    "\n",
    "Also, if the data is missing, we will attempt to download it (from github) and put it in the \"data\" directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib as mplot\n",
    "%matplotlib inline\n",
    "import IPython\n",
    "from IPython.core.display import HTML\n",
    "from IPython.core.debugger import set_trace\n",
    "from distutils.version import StrictVersion\n",
    "print(\"numpy version:  %s\" % np.__version__)\n",
    "print(\"pandas version:  %s\" % pd.__version__)\n",
    "print(\"matplotlib version:  %s\" % mplot.__version__)\n",
    "print(\"IPython version:  %s\" % IPython.__version__)\n",
    "print(\"seaborn version:  %s\" % sns.__version__)\n",
    "\n",
    "if StrictVersion(np.__version__) >= StrictVersion('1.13.0') and \\\n",
    "   StrictVersion(pd.__version__) >= StrictVersion('0.20.0') and \\\n",
    "   StrictVersion(mplot.__version__) >= StrictVersion('2.0.0') and \\\n",
    "   StrictVersion(IPython.__version__) >= StrictVersion('5.5.0') and \\\n",
    "   StrictVersion(sns.__version__) >= StrictVersion('0.7.0'):\n",
    "    print('\\nCongratulations, your environment is setup correctly!')\n",
    "else:\n",
    "    print('\\nEnvironment is NOT setup correctly!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1524775496284,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "AVqO021euWsf",
    "outputId": "51b73eac-399f-45a1-cfa5-62cc7215a373"
   },
   "outputs": [],
   "source": [
    "# Try to install the Excel reader library\n",
    "\n",
    "try:\n",
    "    import xlrd\n",
    "    print('The Excel library is installed.')\n",
    "except ImportError:\n",
    "    print('Installing the Excel library')\n",
    "    !pip install xlrd\n",
    "    import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1524767644385,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "EqZAggVouWsk",
    "outputId": "69de5211-5ebc-413c-97e1-03a2ca002b94"
   },
   "outputs": [],
   "source": [
    "# Find the data directory and download data if it is missing\n",
    "\n",
    "import os, shutil\n",
    "cwd = os.getcwd()\n",
    "datadir = cwd + '/data'\n",
    "\n",
    "print('Data directory is: {}'.format(datadir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1524767644385,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "EqZAggVouWsk",
    "outputId": "69de5211-5ebc-413c-97e1-03a2ca002b94"
   },
   "outputs": [],
   "source": [
    "# See if the data exists.  If not, try to download it from github.\n",
    "if not os.path.exists(datadir):\n",
    "    print(\"Data directory doesn't exist!\")\n",
    "    # Checkout the data from Github\n",
    "    !git clone https://github.com/stevejohnson2001/dswg\n",
    "    shutil.move('dswg/data','.')\n",
    "    shutil.rmtree('dswg')\n",
    "print('Data directory contains:\\n',os.listdir(datadir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZA3W-hhxkpV"
   },
   "source": [
    "# 2.0 Read in the data\n",
    "\n",
    "Now that we have all the libraries installed and the data is available, lets try to read it into Pandas DataFrames.  \n",
    "\n",
    "The first thing we will do is define a Data Dictionary (dd) that describes our expectations for the data.  It includes data types for the columns as well is information about whether the column is required or optional.  The data is read into a dictionary of Dataframes (data) and also assigned to  variables (patients, encounters, etc) for convenience.\n",
    "\n",
    "We will convert dates and other fields to the proper format when later when we do data preparation.\n",
    "\n",
    "The dataset is synthetic data generated by the Synthea project (https://github.com/synthetichealth/synthea).  Synthea creates realistic (but not real) EHR-like data that we can use for demonstrating the techniques of data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1524767660221,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "U3ymTsiKuWsn",
    "outputId": "b5770c83-f14e-4708-a6e7-46728e50fc99"
   },
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "dd = {}\n",
    "\n",
    "dd['patients'] = {'pat_id':     {'type': np.str, 'required':True},  \n",
    "                  'birth_date': {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                  'death_date': {'type': np.datetime64, 'format': '%Y-%m-%d' }, \n",
    "                  'ssn':        {'type': np.str},\n",
    "                  'drivers':    {'type': np.str},\n",
    "                  'passport':   {'type': np.str},\n",
    "                  'prefix':     {'type': np.str},\n",
    "                  'first':      {'type': np.str, 'required':True},\n",
    "                  'last':       {'type': np.str, 'required':True},\n",
    "                  'suffix':     {'type': np.str},\n",
    "                  'maiden':     {'type': np.str},\n",
    "                  'marital':    {'type': np.str},\n",
    "                  'race':       {'type': np.str},\n",
    "                  'ethnicity':  {'type': np.str},\n",
    "                  'gender':     {'type': np.str, 'required':True},\n",
    "                  'birthplace': {'type': np.str},\n",
    "                  'address':    {'type': np.str, 'required':True},\n",
    "                  'prior_opioid_abuse_diag': {'type': np.int}\n",
    "                  }\n",
    "dd['encounters'] = {'enc_id':                 {'type': np.str, 'required':True}, \n",
    "                    'enc_date':               {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                    'enc_pat_id':             {'type': np.str, 'required':True},\n",
    "                    'enc_code':               {'type': np.str, 'required':True},\n",
    "                    'enc_description':        {'type': np.str, 'required':True},\n",
    "                    'enc_reason_code':        {'type': np.str},\n",
    "                    'enc_reason_description': {'type': np.str}\n",
    "                   }\n",
    "dd['observations'] = {'obs_date':        {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                      'obs_pat_id':      {'type': np.str, 'required':True},\n",
    "                      'obs_enc_id':      {'type': np.str, 'required':True},\n",
    "                      'obs_code':        {'type': np.str, 'required':True},\n",
    "                      'obs_description': {'type': np.str, 'required':True},\n",
    "                      'obs_value':       {'type': np.str},\n",
    "                      'obs_units':       {'type': np.str}\n",
    "                     }\n",
    "dd['medications'] = {'med_start_date':         {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                     'med_stop_date':          {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':False},\n",
    "                     'med_pat_id':             {'type': np.str, 'required':True},\n",
    "                     'med_enc_id':             {'type': np.str, 'required':True},\n",
    "                     'med_code':               {'type': np.str, 'required':True},\n",
    "                     'med_description':        {'type': np.str, 'required':True},\n",
    "                     'med_reason_code':        {'type': np.str},\n",
    "                     'med_reason_description': {'type': np.str},\n",
    "                     'med_days_supply':        {'type': np.int}\n",
    "                     }\n",
    "dd['conditions'] =  {'cond_start_date':         {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':True},\n",
    "                     'cond_stop_date':          {'type': np.datetime64, 'format': '%Y-%m-%d', 'required':False},\n",
    "                     'cond_pat_id':             {'type': np.str, 'required':True},\n",
    "                     'cond_enc_id':             {'type': np.str, 'required':True},\n",
    "                     'cond_code':               {'type': np.str, 'required':True},\n",
    "                     'cond_description':        {'type': np.str, 'required':True}\n",
    "                     }\n",
    "\n",
    "\n",
    "# Display the data dictionary\n",
    "\n",
    "for name, tbl_dd in dd.items():\n",
    "    display(HTML('<h2>{}</h2>'.format(name)))\n",
    "    display(pd.DataFrame(tbl_dd).fillna('').T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2377
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1524767660221,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "U3ymTsiKuWsn",
    "outputId": "b5770c83-f14e-4708-a6e7-46728e50fc99"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "                    \n",
    "# Loop through each of the file defintions in our data dictionary\n",
    "for f in dd:\n",
    "    m = dd[f]\n",
    "    col_names = list(m.keys())\n",
    "    data[f] = pd.read_csv(datadir + '/{}.csv'.format(f), dtype=str, index_col=False, header=0, names=col_names, keep_default_na=False)\n",
    "    \n",
    "patients = data['patients']\n",
    "encounters = data['encounters']\n",
    "observations = data['observations']\n",
    "medications = data['medications']\n",
    "conditions = data['conditions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the list of opioid medications\n",
    "\n",
    "It will be important to know which of the medications that are prescribed are considered opioids.  The UMLS VSAC maintains value set lists of which medications are considered opioids.  You can download the current list by going to https://vsac.nlm.nih.gov/ and searching for opioid value sets.  We have downloaded the list called \"All prescribable opioids used for pain control including Inactive Medications\" (oid 1.3.6.1.4.1.6997.4.1.2.234.999.3.2) into the data directory.  We will read the Excel file and pull out the codes from the second sheet in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Opioid code list from VSAC\n",
    "# oid 1.3.6.1.4.1.6997.4.1.2.234.999.3.2\n",
    "xl = pd.ExcelFile(datadir + '/AllPrescribableOpioidsUsedForPainControlIncludingInactiveMedications.xlsx')\n",
    "df = xl.parse(\"Code List\", skiprows=12)\n",
    "display(df.head(10))\n",
    "opioids_rxnorm = list(df['Code'].astype(np.str))  # Make sure the codes are treated as strings\n",
    "\n",
    "\n",
    "#TODO opioids_rxnorm = ['1049369','1310197','1049544']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwLnaiLiLjba"
   },
   "source": [
    "# 3.0 Exploratory Data Analysis - Part 1\n",
    "\n",
    "Start by looking at the data to see what types of values are in each variables, the relationships and get a feel for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start by display the data as Dataframes\n",
    "\n",
    "Displaying the first 5 rows of the data is a good way to look for obvious issues before working with the data in more detail.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each of the file defintions in our data dictionary\n",
    "for f in dd:\n",
    "    m = dd[f]\n",
    "    display(HTML('<h3>{}, {} records</h3>'.format(f,len(data[f]))))\n",
    "    display(data[f].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2245,
     "status": "error",
     "timestamp": 1524768116330,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "Oxb4DG_nMDwv",
    "outputId": "44f25bc9-f78d-4aaa-b036-927f6519341f"
   },
   "outputs": [],
   "source": [
    "# Look at the categorical variables\n",
    "\n",
    "sns.countplot(x='race', data=patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 609
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2245,
     "status": "error",
     "timestamp": 1524768116330,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "Oxb4DG_nMDwv",
    "outputId": "44f25bc9-f78d-4aaa-b036-927f6519341f"
   },
   "outputs": [],
   "source": [
    "g = sns.countplot(x='ethnicity', data=patients)\n",
    "z = plt.xticks(rotation=-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For continuous variables, we can graph the distribution\n",
    "\n",
    "w = observations[observations['obs_code']=='29463-7']   # Find all of the \"weight\" observations\n",
    "weights = w['obs_value'].astype(np.float)\n",
    "mean = np.mean(weights)\n",
    "print('Average weight: ',mean)\n",
    "\n",
    "# Plot the distribution\n",
    "sns.distplot(weights)\n",
    "plt.xlabel(\"WEIGHT (kg)\")\n",
    "plt.show()   # If you don't explicitly \"show\" the plot, Jupyter will automatically show the last plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As an Exercise, have participants graph the Height of patients\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Data Preparation\n",
    "\n",
    "Now that we know a little about our data, we can begin to prepare it for analysis.  We need to:\n",
    "1. Find data that is not formatted correctly\n",
    "2. Deal with missing data\n",
    "\n",
    "In all of these cases, we will have to decide what to do with the bad data.  We can:\n",
    "1. Delete the data\n",
    "2. Impute a reasonable value for the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DQ quality checks and print graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: Move to CreateData\n",
    "# # Make some data missing and bad format\n",
    "# from random import randint\n",
    "# for name, tbl_dd in dd.items():\n",
    "#     print('Name: ',name)\n",
    "#     d = data[name]\n",
    "#     for field_name, field in tbl_dd.items():\n",
    "#         if field['type'] != np.str:   # Only inject errors in non-string types\n",
    "#             print('  Field: ', field_name)\n",
    "#             col = d[field_name]\n",
    "#             to_change = np.random.randint(0,len(col),randint(0,5))\n",
    "#             print('Changing Format ',len(to_change),to_change)\n",
    "#             col.loc[to_change] = 'BAD_DATA'\n",
    "#             col = d[field_name]\n",
    "#             to_change = np.random.randint(0,len(col),randint(0,5))\n",
    "#             print('Changing Missing ',len(to_change),to_change)\n",
    "#             col.loc[to_change] = ''    \n",
    "#     display(d.head(10))\n",
    "# # change = patients.sample(5).index\n",
    "# # patients.loc[change,'birth_date'] = 'BAD_DATE'\n",
    "# # change = [4,5,6] #patients.sample(5).index\n",
    "# # patients.loc[change,'birth_date'] = ''\n",
    "# # print(np.where(patients['birth_date'] == 'BAD_DATE'))\n",
    "# # print(len(np.where(patients['birth_date'] == 'BAD_DATE')[0]))\n",
    "# # display(patients.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find data that is not formatted correctly\n",
    "# TODO: Clean this up\n",
    "\n",
    "def parse_date(dt,fmt):\n",
    "    if type(dt) == str and (dt == ''):\n",
    "        return dt\n",
    "    try:\n",
    "        return pd.to_datetime(dt,format=fmt)\n",
    "    except:\n",
    "        return np.datetime64('NaT')\n",
    "\n",
    "def parse_int(num):\n",
    "    if type(num) == str and (num == ''):\n",
    "        return num\n",
    "    try:\n",
    "        return int(num)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# Loop through our Data Dictionary\n",
    "for name, tbl_dd in dd.items():\n",
    "    display(HTML('<h2>{}</h2>'.format(name)))\n",
    "    d = data[name]\n",
    "    for field_name, field in tbl_dd.items():\n",
    "        #print('  Field: ', field_name)\n",
    "        col = d[field_name]\n",
    "        #print(col.head(5))\n",
    "        field['DQ'] = {}\n",
    "        field['DQ']['missing'] = len(np.where(col == '')[0])\n",
    "        #print('    Missing = ',field['DQ']['missing'])\n",
    "\n",
    "        if field['type'] == np.datetime64:\n",
    "            #print('   Date')\n",
    "            if 'format' in field:\n",
    "                fmt = field['format']\n",
    "            else:\n",
    "                fmt = '%Y-%m-%d'\n",
    "            #pd.to_datetime(col, format=fmt, errors='ignore')\n",
    "            d[field_name] = col.apply(lambda x: parse_date(x,fmt))\n",
    "            field['DQ']['format_errors'] = col.isnull().sum()\n",
    "            #print('    Bad format = ',field['DQ']['format_errors'])\n",
    "            #display(col.head(5))\n",
    "        elif field['type'] == np.int:\n",
    "            #print('   Int')\n",
    "            d[field_name] = col.apply(lambda x: parse_int(x))\n",
    "            field['DQ']['format_errors'] = col.isnull().sum()\n",
    "            #print('    Bad format = ',field['DQ']['format_errors'])\n",
    "        elif field['type'] == np.str:\n",
    "            pass # Everything is valid syntax\n",
    "            #print('   Str')\n",
    "            field['DQ']['format_errors'] = 0\n",
    "            \n",
    "    # Show the Data Quality information\n",
    "    display(pd.DataFrame(dd[name]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display table showing total records, missing data, bad format\n",
    "# TODO: Delete?\n",
    "\n",
    "for name, tbl_dd in dd.items():\n",
    "    print('Name: ',name)\n",
    "    d = data[name]\n",
    "    display(tbl_dd)\n",
    "    dql = []\n",
    "    for name, col in tbl_dd.items():\n",
    "        dq = col['DQ']\n",
    "        dql.append([name,len(d[name]),dq['format_errors'],dq['missing']])\n",
    "    dqf = pd.DataFrame(dql,columns=['field_name','records','format_errors','missing'])\n",
    "    display(dqf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Move this\n",
    "display(patients.isnull().head(5))\n",
    "display(patients.isnull().any())\n",
    "null_columns=patients.columns[patients.isnull().any()]\n",
    "print(null_columns)\n",
    "patients[null_columns].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dealing with missing data, delete\n",
    "\n",
    "# dropna\n",
    "# If required and data is missing, drop the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dealing with missing data, imputation\n",
    "# http://www.data-mania.com/blog/logistic-regression-example-in-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the .isnull() function and the .any() function to find all rows with missing data\n",
    "missing_data = patients[patients.isnull().any(axis=1)]\n",
    "display(missing_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop missing or incorrectly formated data\n",
    "missing = patients[patients.isnull().any(axis=1)].index\n",
    "print(\"Missing = \",missing)\n",
    "patients.drop(missing,inplace=True)\n",
    "\n",
    "# Make sure they are gone\n",
    "missing = patients[patients.isnull().any(axis=1)].index\n",
    "print(\"Missing = \",missing)\n",
    "\n",
    "print(patients.shape)\n",
    "\n",
    "\n",
    "# TODO: Do for all Dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVVDeJqpMSK6"
   },
   "source": [
    "## 4.1 Transform the Data\n",
    "\n",
    "Use the power of Pandas Dataframes to transform the data.  Add new columns as calculations from existing columns, join the data together and get it into the format you need for analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the working dataframe\n",
    "# TODO cleanup\n",
    "#df = encounters.merge(patients, how='left', left_on='enc_pat_id', right_on='pat_id')\n",
    "df = patients\n",
    "#df['age_at_visit'] = round((pd.to_datetime(df['enc_date']) - pd.to_datetime(df['birth_date'])).dt.days/365)\n",
    "df['age'] = round((pd.Timestamp.today() - pd.to_datetime(patients['birth_date'])).dt.days/365)\n",
    "df['adult'] = np.where(df['age'] >= 18, 1, 0)\n",
    "\n",
    "#patients['age'] = round((pd.Timestamp.today() - pd.to_datetime(patients['birth_date'])).dt.days/365)\n",
    "\n",
    "# Use a set to eliminate duplicates\n",
    "patients_that_overdosed = set(encounters[encounters['enc_reason_code']=='55680006']['enc_pat_id'])  # Overdose\n",
    "\n",
    "df['overdose'] = np.where(df['pat_id'].isin(patients_that_overdosed), 1, 0)\n",
    "#patients['overdose'] = np.where(patients['pat_id'].isin(patients_that_overdosed), 1, 0)\n",
    "\n",
    "patients_overdose_deaths = set(observations[(observations['obs_code'] == '69453-9') &   # Death\n",
    "                                (observations['obs_value'].str.contains('overdose'))]['obs_pat_id'])\n",
    "\n",
    "\n",
    "patients_prescribed_opioids = set(medications[medications['med_code'].isin(opioids_rxnorm)]['med_pat_id'])  # Opioids\n",
    "\n",
    "df['prescribed_opioids'] = np.where(df['pat_id'].isin(patients_prescribed_opioids), 1, 0)\n",
    "#patients['prescribed_opioids'] = np.where(patients['pat_id'].isin(patients_prescribed_opioids), 1, 0)\n",
    "\n",
    "print('Num patients prescribed opioids = {}, Num overdoses = {}, Num overdose deaths = {}'\n",
    "         .format(len(patients_prescribed_opioids),len(patients_that_overdosed),len(patients_overdose_deaths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXdxuyM5MeIG"
   },
   "source": [
    "### 4.2.1 Compute the days_supply variable\n",
    "\n",
    "We want to compute how many days supply of a medication a patient was prescribed at discharge.  The approach we will use is that for each encounter, we will find all of the medications associated with the encounter.  We will look for medicates that are opioids and find the largest days supply for that encounter and store the result in the 'opioid_discharge_days_supply' column.\n",
    "\n",
    "This is most easily accomlished using a function that defines the logic and the \".apply\" DataFrame function the will iterate over each row in a DataFrame, call the function and store the result back in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that will perform to logic of compute the discharge opioid days supply\n",
    "from random import random\n",
    "# TODO Fix in CreateData\n",
    "def get_days_supply(pat_id):\n",
    "    enc_meds = medications[medications['med_pat_id'] == pat_id]\n",
    "    enc_opioid_meds = enc_meds[enc_meds['med_code'].isin(opioids_rxnorm)]\n",
    "    max = 0\n",
    "    if len(enc_opioid_meds) > 0:\n",
    "        try:\n",
    "            max = int(enc_opioid_meds['med_days_supply'].max())\n",
    "        except ValueError:\n",
    "            max = 0\n",
    "    return int(max)\n",
    "\n",
    "# Apply the function to each row (Note: this can take a little while to finish)\n",
    "df['opioid_discharge_days_supply'] = df.apply(lambda x: get_days_supply(x['pat_id']), axis=1)\n",
    "\n",
    "# Display the first 5 entries that have a non-zero days supply, just to check our logic\n",
    "df[df['opioid_discharge_days_supply'] > 0].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Explore the Data - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ygRWbg6_QHur"
   },
   "source": [
    "### 5.1 Outcome variable\n",
    "\n",
    "Now that we have created some new variables, lets take a look at how the outcome variable is associated with our predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does the outcome variable look like\n",
    "#    We need to compute some of our variables of interest \n",
    "#  Compute \"overdose\", \"age\", \"adult\", \"prescribed_opioids\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome variable\n",
    "\n",
    "Lets see what the outcome variable looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See who overdosed from prescribed opioids \n",
    "# There are 7 patients that \n",
    "display(patients_that_overdosed.intersection(patients_prescribed_opioids))\n",
    "display(patients[patients['pat_id'].isin(patients_that_overdosed)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many patients overdosed?\n",
    "\n",
    "Since we store 'overdose' as a 0 or 1, we can just use the mean function to compute what percent of the population overdosed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(patients['overdose'].value_counts())\n",
    "print('Percent that overdosed: {0:.2f}%'.format(patients['overdose'].mean()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.groupby('overdose').mean()\n",
    "\n",
    "# Why were only 6 patients prescribed opioids that overdosed?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What was the mean number of days_supply for patients that overdosed and were prescribed opioids?\n",
    "\n",
    "ct = pd.crosstab(df['prescribed_opioids'],df['opioid_discharge_days_supply'])\n",
    "ct.iloc[1][1:].plot()\n",
    "#sns.factorplot(data=ct[0], x='')\n",
    "df[['overdose','opioid_discharge_days_supply']].groupby('overdose').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph the patient variables against the outcome\n",
    "\n",
    "Let's see if gender, race and age are associated with the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(patients['gender'],patients['overdose']).plot(kind='bar')\n",
    "plt.title('Overdose by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Overdose?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Male vs Female overdoses\n",
    "display(pd.crosstab(patients['gender'],patients['overdose'],margins=True))\n",
    "display(pd.crosstab(patients['race'],patients['overdose'],margins=True))\n",
    "display(pd.crosstab(patients['race'],patients['overdose'],normalize='index'))\n",
    "pd.crosstab(patients['ethnicity'],patients['overdose'],normalize='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(patients['race'],patients['overdose']).plot(kind='bar')\n",
    "plt.title('Overdose by Race')\n",
    "plt.xlabel('Race')\n",
    "plt.ylabel('Overdose?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age when they visited is pretty uniform until age 60 when it starts to taper off. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "patients['age'].hist()\n",
    "plt.title('Histogram of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph age_at_visit with probability of overdose\n",
    "# TODO: FIX\n",
    "\n",
    "ct = pd.crosstab(df['age'],df['overdose'],normalize='index')[1]\n",
    "\n",
    "#ct.hist()\n",
    "ct.plot()\n",
    "plt.title('Histogram of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Probability of Overdose')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by a variable\n",
    "\n",
    "We often want to group related rows together and then count the number rows of each type or find the mean of a variable for each row type.\n",
    "\n",
    "For example, lets count how many encounters each patient has over the timeframe of the data.  We will use the `groupby` function to group on a set of variables.  The operation returns a `groupby` object which doesn't actually group the data but instead acts like a set of instructions telling the DataFrame how to group itself.  We need to apply another function, such as size(), mean() or sum(), to the groups to yield a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EAW0QqbkLrJh"
   },
   "outputs": [],
   "source": [
    "# How many encounters does each patient have?\n",
    "\n",
    "encs = encounters.groupby(['enc_pat_id']).size()\n",
    "\n",
    "#display(encs[0:5])\n",
    "#display(type(encs))\n",
    "\n",
    "# We can store that information directly into the patients DataFrame since `encs` is indexed by the pat_id\n",
    "patients = patients.set_index('pat_id')\n",
    "patients['num_encounters'] = encs\n",
    "patients = patients.reset_index()\n",
    "\n",
    "#display(patients.head(5))\n",
    "\n",
    "sns.lmplot(data=patients,x='age',y='num_encounters',hue='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For those that overdose, what is the days_supply?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df['overdose']==1].mean())\n",
    "df[df['prescribed_opioids']==1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the primary reasons for visit for those that overdosed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounters[encounters['enc_pat_id'].isin(patients_that_overdosed)]['enc_reason_description'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For patients that overdosed, what was the most frequent condition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conditions[conditions['cond_pat_id'].isin(patients_that_overdosed)]['cond_description'].value_counts(sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an idea of how a patient progresses through their healthcare\n",
    "\n",
    "When exploring the data, it helps to visualize what is happening across time.  You can create small functions within the Jupyter notebook and reuse them further down in the notebook.  In this case, we are looping through the encounter data for a patient and print all of the medications and labs (observations) that are associated with the patient.  The function 'display_trajectory' is passed the id for a patient and then prints the information.  We can use this later to further examine data or debug things we don't understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1292
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 815,
     "status": "ok",
     "timestamp": 1524772350197,
     "user": {
      "displayName": "Steve Johnson",
      "photoUrl": "//lh3.googleusercontent.com/-aWoUrtEbcoA/AAAAAAAAAAI/AAAAAAAAAgw/MZkRYHRUBoo/s50-c-k-no/photo.jpg",
      "userId": "111376329577814829723"
     },
     "user_tz": 300
    },
    "id": "kNlJlgaCY-O2",
    "outputId": "a95e8d54-e57f-4d87-85ac-98b1f01df261"
   },
   "outputs": [],
   "source": [
    "pt_id = '3eaed230-1c60-4221-a96c-f6af5d871072'\n",
    "#pt = patients.query('pat_id == @pt_id')\n",
    "#pt = patients[patients.pat_id == pt_id].iloc[0]\n",
    "#pt = patients.loc[pt_id]\n",
    "\n",
    "def display_trajectory(df,pt_id):\n",
    "    pt = patients[patients['pat_id']==pt_id]\n",
    "\n",
    "    display(pt)\n",
    "    encs = encounters[encounters.enc_pat_id == pt_id]\n",
    "    #print(encs.shape)\n",
    "    for i, e in encs.iterrows():\n",
    "        #dt = df[df['pat_id']==e['enc_pat_id']].iloc[0]\n",
    "        print('  {:%Y-%m-%d}: {} ({}) ({})'.format(e['enc_date'], e['enc_description'], \\\n",
    "                         e['enc_code'], e['enc_reason_description']))\n",
    "        meds = medications[medications['med_enc_id'] == e['enc_id']]\n",
    "        for j, m in meds.iterrows():\n",
    "            print('     MED: {:%Y-%m-%d}: {} ({}) days_supply={}'.format(m['med_start_date'],  \\\n",
    "                                            m['med_description'], m['med_code'], m['med_days_supply']))\n",
    "        labs = observations[observations['obs_enc_id'] == e['enc_id']]\n",
    "        for k, l in labs.iterrows():\n",
    "            print('     LAB: {:%Y-%m-%d %H:%M}: {} ({}) {} {}'.format(l['obs_date'], l['obs_description'], l['obs_code'], l['obs_value'], l['obs_units']))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_trajectory(df,patients.iloc[0].pat_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Who were the patients that overdosed?\n",
    "\n",
    "pt = df[df['pat_id'].isin(patients_that_overdosed)]\n",
    "\n",
    "pt.head(10)\n",
    "#display(pd.DataFrame([pt['first'],pt['last'],pt['marital'],pt['race'],pt['gender'],pt['ethnicity']]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the trajectory of one of those patients\n",
    "pt_id = list(patients_that_overdosed)[1]\n",
    "display_trajectory(df,pt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from random import randint, random\n",
    "# # Add in days_supply\n",
    "# for i, enc in df.iterrows():\n",
    "#     if enc['prescribed_opioids']:\n",
    "#         days_supply = randint(2,15)\n",
    "#         if enc['overdose']:\n",
    "#             if random() > 0.30:   # Overdose patients have a 70% chance of a large days supply\n",
    "#                 days_supply = randint(20,60)      \n",
    "#     else:\n",
    "#         days_supply = 0  # They were not prescribed opioids\n",
    "#     df[enc.name,'opioid_discharge_days_supply'] = days_supply\n",
    "# display(df[df['overdose']==1].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# med2 = pd.DataFrame()\n",
    "# for i, med in medications.iterrows():\n",
    "#     days_supply = randint(2,15)\n",
    "#     overdose = med['med_pat_id'] in patients_that_overdosed\n",
    "#     opioid = med['med_code'] in opioids_rxnorm\n",
    "#     if overdose & opioid:\n",
    "#         if random() > 0.30:   # Overdose patients have a 70% chance of a large days supply\n",
    "#             days_supply = randint(20,60)      \n",
    "#     med['days_supply'] = days_supply\n",
    "#     med2.append(med)\n",
    "# #     print(med)\n",
    "# #     print(med2.shape)\n",
    "# #     %%debug\n",
    "# med2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Modeling\n",
    "\n",
    "We will be using the `scikit-learn` package, which is an open-source library that provides a robust set of machine learning algorithms for Python. It is built upon the core Python scientific stack and has a simple, consistent interface.\n",
    "\n",
    "<img src=\"http://1.bp.blogspot.com/-ME24ePzpzIM/UQLWTwurfXI/AAAAAAAAANw/W3EETIroA80/s1600/drop_shadows_background.png\" width=\"800px\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the modules\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import random as rnd\n",
    "from random import random, randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the models in scikit-learn require the categorical variables be turned into numeric variables.  There are two approaches to this:\n",
    "1. One Hot Encoding - each item in the categorical variable is turned into its own variable represetnting the presence or abscence of that item.  For example, 'Gender' would turn into 2 variables:  'Gender_M' and 'Gender_F'\n",
    "2. Label Encoding - assign an integer to each item.  For the 'Gender' variable, 0 might mean Male and 1 might mean Female.\n",
    "\n",
    "We will use the `LabelEncoder` transformation to change categorical variables into integers.  As a convenience, we can also keep the original (human readable) variable in the Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the following variables as our initial set of predictors\n",
    "cat_cols = ['gender', 'marital', 'race', 'ethnicity']\n",
    "cat_cols_encoded = [c + '_encoded' for c in cat_cols]\n",
    "numeric_cols = ['prior_opioid_abuse_diag', 'age', 'opioid_discharge_days_supply']\n",
    "pred_cols = numeric_cols + cat_cols_encoded\n",
    "target_col = 'overdose'\n",
    "all_cols = cat_cols+numeric_cols+[target_col]\n",
    "\n",
    "df_opioids = df[df['prescribed_opioids'] == 1]\n",
    "print(df_opioids.shape)\n",
    "\n",
    "# Encode the categorical variables\n",
    "dfe = df_opioids[cat_cols]\n",
    "print(dfe.shape)\n",
    "# Replace missing data with an 'Unknown' category so the missing data will also be encoded\n",
    "dfe = dfe.replace(np.NaN,'Unknown')\n",
    "\n",
    "# Encode the categorical variables\n",
    "encoded = dfe.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "\n",
    "# Append the non-categorical variables and the encoded variables into a single Dataframe\n",
    "# Name the new variables as <name>_encoded\n",
    "dfe = pd.concat([df_opioids[all_cols], encoded.add_suffix('_encoded')],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_opioids.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to build a model using this set of variables\n",
    "pred_cols = ['age', #'opioid_discharge_days_supply', \n",
    "             'prior_opioid_abuse_diag', \\\n",
    "             'gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded']\n",
    "#pred_cols = ['prior_abuse_diag', 'adult', 'age_at_visit', 'opioid_discharge_days_supply', \\\n",
    "#             'gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded']\n",
    "\n",
    "LR_pred_cols = pred_cols\n",
    "X = dfe[pred_cols].as_matrix()\n",
    "y = dfe['overdose'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remind us of the definitions of recall (sensitivity) and precision (positive predictive value), here is a graphic from Wikipedia:\n",
    "\n",
    "![Confusion Matrix](Sensitivity-Wikipedia.png)\n",
    "\n",
    "To quote from Scikit Learn:\n",
    "\n",
    "The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier to not label a sample as positive if it is negative.\n",
    "\n",
    "The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "\n",
    "Accuracy is (tp + tn) / N.\n",
    "\n",
    "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
    "\n",
    "The F-beta score weights the recall more than the precision by a factor of beta. beta = 1.0 means recall and precision are equally important.\n",
    "\n",
    "The support is the number of occurrences of each class in y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try a simple logistic regression model to see how predictive our data is\n",
    "\n",
    "# https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\n",
    "# \n",
    "\n",
    "LR = LogisticRegression()\n",
    "result = LR.fit(X,y)\n",
    "\n",
    "expected = y\n",
    "predicted = LR.predict(X)\n",
    "\n",
    "print('\\nClassification Report\\n',metrics.classification_report(expected, predicted))\n",
    "print('\\nConfusion Matrix\\n',metrics.confusion_matrix(expected, predicted))\n",
    "print('\\nAccuracy score =',metrics.accuracy_score(expected, predicted))\n",
    "print('\\nAUC score =',metrics.roc_auc_score(expected, predicted))\n",
    "print('\\nf1 score =',metrics.f1_score(expected, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets graph an ROC curve for the model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "auc = roc_auc_score(y, LR.predict(X))\n",
    "\n",
    "probs = LR.predict_proba(X)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y, probs)\n",
    "tpr[1] = tpr[0]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Model')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC1.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets try it with Dummy Variables (One Hot Coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Model Evaluation\n",
    "\n",
    "We built a Logistic Regression model that was moderatly predictive of an overdose.  But we trained the model and tested it with our entire data set, which isn't exactly a fair test of the model.\n",
    "\n",
    "We can split our dataset into a training dataset and a test dataset.  After we train the model using just the training dataset, we will evaluate the model using the test dataset which has data that the model has never seen before.\n",
    "\n",
    "scikit-learn has a nice function called `train_test_split` that randomly splits our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training and test datasets\n",
    "train, test = train_test_split(dfe, test_size=0.3, random_state=987)\n",
    "\n",
    "X_train = train[pred_cols].as_matrix()\n",
    "y_train = train['overdose'].as_matrix()\n",
    "X_test = test[pred_cols].as_matrix()\n",
    "y_test = test['overdose'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression()\n",
    "result = LR.fit(X_train,y_train)\n",
    "print(LR)\n",
    "expected = y_train\n",
    "predicted = LR.predict(X_train)\n",
    "print(metrics.classification_report(expected, predicted))\n",
    "print(metrics.confusion_matrix(expected, predicted))\n",
    "\n",
    "expected = y_test\n",
    "predicted = LR.predict(X_test)\n",
    "\n",
    "print('\\nClassification Report\\n',metrics.classification_report(expected, predicted))\n",
    "print('\\nConfusion Matrix\\n',metrics.confusion_matrix(expected, predicted))\n",
    "print('\\nAccuracy score =',metrics.accuracy_score(expected, predicted))\n",
    "print('\\nAUC score =',metrics.roc_auc_score(expected, predicted))\n",
    "print('\\nf1 score =',metrics.f1_score(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets graph an ROC curve for the model\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "auc = roc_auc_score(y_test, LR.predict(X_test))\n",
    "fpr, tpr, thresholds = roc_curve(y_test, LR.predict_proba(X_test)[:,1])\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % auc)\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Model (train/test split)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n",
    "\n",
    "Splitting the data into a training dataset and a test dataset has a drawback in that it doesn't allow us to train the model on all of the data.  Another approach is to use k-fold cross validation where we randomly divide the data into k equal sized subsets and then train the model on the remaining k-1 segments of the data and test it on the data that we held out.  The process is repeated k times so that in the end all of the data is used at some point to train the model.  The k model results are averaged to produce a single estimate of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=387)\n",
    "results = model_selection.cross_val_score(LR, X_train, y_train, cv=kfold, scoring='roc_auc')\n",
    "\n",
    "print('Model score = %.4f (%.4f)' %(results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing many models\n",
    "\n",
    "The nice thing about scikit-learn is that most of the models have similar calling signatures so we can just create a list of all of the models we are interested in testing and then invoke each model in a for loop.\n",
    "\n",
    "We can train each model in turn and then use k-fold cross validate to estimate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare configuration for cross validation test harness \n",
    "# (from https://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/)\n",
    "\n",
    "# Make this into a function so we can reuse it later\n",
    "\n",
    "def eval_models(X, y):\n",
    "    seed = 543\n",
    "    # prepare models\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression(class_weight='balanced')))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    #models.append(('SVM', SVC()))\n",
    "    models.append(('RF', RandomForestClassifier(n_estimators=100, class_weight='balanced')))\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    names = []\n",
    "    #scoring = 'accuracy'\n",
    "    scoring = 'roc_auc' # others include: 'accuracy', 'f1', 'roc_auc', \n",
    "                        # or found here: http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "        \n",
    "    return results, names\n",
    "\n",
    "# Execute the function\n",
    "results, names = eval_models(X_train,y_train)  #TODO:  X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphically viewing model performance using a boxplot\n",
    "\n",
    "None of the models do very well.  LogisticRegression does the best but most of the models have quite a bit of variance in their scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "def plot_perf(names,results):\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot(results)\n",
    "    plt.ylabel('roc_auc')\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.show()\n",
    "    \n",
    "plot_perf(names,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we improve the model by adding some variables?\n",
    "new_cols = ['prior_opioid_abuse_diag',  'age',  \\\n",
    "             'gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded']\n",
    "print(new_cols)\n",
    "\n",
    "X = dfe[new_cols].as_matrix()\n",
    "y = dfe['overdose'].as_matrix()\n",
    "\n",
    "X_train = train[new_cols].as_matrix()\n",
    "y_train = train['overdose'].as_matrix()\n",
    "X_test = test[new_cols].as_matrix()\n",
    "y_test = test['overdose'].as_matrix()\n",
    "\n",
    "results, names = eval_models(X,y)\n",
    "\n",
    "plot_perf(names,results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display relative importance of each variable\n",
    "pred_cols = ['gender_encoded', 'marital_encoded', 'race_encoded', 'ethnicity_encoded', 'age', \n",
    "               'prior_opioid_abuse_diag', 'opioid_discharge_days_supply']\n",
    "model = RandomForestClassifier()\n",
    "model.fit(dfe[pred_cols], dfe['overdose'])\n",
    "\n",
    "# display the relative importance of each attribute\n",
    "display(pd.DataFrame(list(zip(pred_cols,model.feature_importances_))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dfe[LR_pred_cols].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how to use the resulting model to predict opioid overdose\n",
    "\n",
    "new_patient = [45,1,0,1,4,7]\n",
    "\n",
    "pred = LR.predict(np.asmatrix(new_patient))\n",
    "if pred[0] == 0:\n",
    "    print('No overdose risk.')\n",
    "elif pred[0] == 1:\n",
    "    print('Overdose risk.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "AMIA-Workshop-v4.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
